#!/usr/bin/env python3
"""
é»éƒ¨è½æ–‡ç« é·ç§»è…³æœ¬
å¾ https://www.dotblogs.com.tw/jakeuj/ çˆ¬å–æ–‡ç« ä¸¦è½‰æ›ç‚º Markdown æ ¼å¼
"""

import requests
from bs4 import BeautifulSoup
import os
import re
import json
from datetime import datetime
from urllib.parse import urljoin, urlparse
import time
from markdownify import markdownify as md

# é…ç½®
BASE_URL = "https://www.dotblogs.com.tw/jakeuj/"
OUTPUT_DIR = "Writerside/topics/dotblog"
IMAGES_DIR = "Writerside/images/dotblog"
TOTAL_PAGES = 1  # å¯¦éš›ä¸Šæ¯é éƒ½é¡¯ç¤ºç›¸åŒçš„æ–‡ç« ,åªéœ€çˆ¬å–ç¬¬1é 

# æŠ€è¡“åˆ†é¡æ˜ å°„
CATEGORY_MAPPING = {
    'ABP': 'ABP',
    'ASP.NET': 'C-Sharp',
    'C#': 'C-Sharp',
    '.NET': 'C-Sharp',
    'Azure': 'Azure',
    'GCP': 'gcp',
    'Docker': 'Docker',
    'Git': 'Git',
    'PowerShell': 'PowerShell',
    'Flutter': 'Flutter',
    'Python': 'Python',
    'SQL': 'SQL',
    'Machine Learning': 'LLM',
    'ML': 'LLM',
    'Jetbrains': 'JetBrains',
    'JetBrains': 'JetBrains',
    'Rider': 'JetBrains',
    'Windows': 'Windows',
    'Ubuntu': 'Ubuntu',
    'WSL': 'WSL',
    'Node.js': 'Node-js',
    'iOS': 'Flutter',
    'Android': 'Flutter',
}

def sanitize_filename(title):
    """å°‡æ¨™é¡Œè½‰æ›ç‚ºåˆæ³•çš„æª”æ¡ˆåç¨±"""
    # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
    filename = re.sub(r'[<>:"/\\|?*]', '', title)
    # æ›¿æ›ç©ºæ ¼å’Œå…¶ä»–å­—ç¬¦
    filename = re.sub(r'\s+', '-', filename)
    # ç§»é™¤å¤šé¤˜çš„ç ´æŠ˜è™Ÿ
    filename = re.sub(r'-+', '-', filename)
    # é™åˆ¶é•·åº¦
    if len(filename) > 100:
        filename = filename[:100]
    return filename.strip('-')

def download_image(img_url, article_id):
    """ä¸‹è¼‰åœ–ç‰‡ä¸¦è¿”å›æœ¬åœ°è·¯å¾‘"""
    try:
        response = requests.get(img_url, timeout=10)
        response.raise_for_status()
        
        # è§£æåœ–ç‰‡æª”å
        parsed_url = urlparse(img_url)
        img_name = os.path.basename(parsed_url.path)
        if not img_name:
            img_name = f"image_{int(time.time())}.png"
        
        # ç¢ºä¿åœ–ç‰‡ç›®éŒ„å­˜åœ¨
        os.makedirs(IMAGES_DIR, exist_ok=True)
        
        # ä¿å­˜åœ–ç‰‡
        img_filename = f"article_{article_id}_{img_name}"
        img_path = os.path.join(IMAGES_DIR, img_filename)
        
        with open(img_path, 'wb') as f:
            f.write(response.content)
        
        # è¿”å›ç›¸å°æ–¼ Writerside çš„è·¯å¾‘
        return f"dotblog/{img_filename}"
    except Exception as e:
        print(f"  âš ï¸  ä¸‹è¼‰åœ–ç‰‡å¤±æ•—: {img_url} - {e}")
        return img_url

def html_to_markdown(html_content, article_url):
    """å°‡ HTML å…§å®¹è½‰æ›ç‚º Markdown"""
    soup = BeautifulSoup(html_content, 'html.parser')

    # ç§»é™¤è…³æœ¬ã€æ¨£å¼å’Œå»£å‘Š
    for element in soup(["script", "style", "iframe", "noscript"]):
        element.decompose()

    # ç§»é™¤å»£å‘Šç›¸é—œçš„ div
    for ad in soup.find_all('div', class_=re.compile(r'ad|advertisement', re.I)):
        ad.decompose()

    # ä½¿ç”¨ markdownify è½‰æ›
    markdown_text = md(str(soup), heading_style="ATX")

    # æ¸…ç†å¤šé¤˜çš„ç©ºè¡Œ
    lines = []
    prev_empty = False
    for line in markdown_text.split('\n'):
        line = line.rstrip()
        if line:
            lines.append(line)
            prev_empty = False
        elif not prev_empty:
            lines.append('')
            prev_empty = True

    return '\n'.join(lines)

def get_article_links_from_page(page_num):
    """å¾åˆ—è¡¨é é¢ç²å–æ‰€æœ‰æ–‡ç« é€£çµ"""
    if page_num == 1:
        url = BASE_URL
    else:
        url = f"{BASE_URL}?page={page_num}"

    print(f"\nğŸ“„ æ­£åœ¨çˆ¬å–ç¬¬ {page_num} é : {url}")

    try:
        response = requests.get(url, timeout=15)
        response.raise_for_status()

        soup = BeautifulSoup(response.content, 'html.parser')

        # æ‰¾åˆ°æ‰€æœ‰æ–‡ç« é€£çµ
        article_links = []

        # å°‹æ‰¾æ–‡ç« å€å¡Š - æ ¹æ“šå¯¦éš› HTML çµæ§‹èª¿æ•´
        articles = soup.find_all('div', class_='post-item') or soup.find_all('article')

        if not articles:
            # å˜—è©¦å…¶ä»–å¯èƒ½çš„é¸æ“‡å™¨
            articles = soup.find_all('div', class_='post')

        for article in articles:
            # å°‹æ‰¾æ–‡ç« æ¨™é¡Œé€£çµ
            title_link = article.find('a', href=re.compile(r'/jakeuj/\d{4}/\d{2}/\d{2}/'))

            if not title_link:
                # å˜—è©¦å…¶ä»–å¯èƒ½çš„é€£çµæ ¼å¼
                title_link = article.find('a', href=re.compile(r'/jakeuj/'))

            if title_link:
                href = title_link.get('href')
                if href and '/jakeuj/' in href and not href.endswith('/jakeuj/'):
                    full_url = urljoin(BASE_URL, href)
                    title = title_link.get_text().strip()

                    # æå–æ—¥æœŸ
                    date_elem = article.find('time') or article.find(string=re.compile(r'\d{4}-\d{2}-\d{2}'))
                    date = date_elem if isinstance(date_elem, str) else (date_elem.get_text().strip() if date_elem else "")

                    article_links.append({
                        'url': full_url,
                        'title': title,
                        'date': date
                    })

        print(f"  âœ“ æ‰¾åˆ° {len(article_links)} ç¯‡æ–‡ç« ")
        return article_links

    except Exception as e:
        print(f"  âŒ çˆ¬å–å¤±æ•—: {e}")
        return []

def fetch_article(article_info):
    """çˆ¬å–å–®ç¯‡æ–‡ç« """
    url = article_info['url']
    print(f"\nğŸ“– æ­£åœ¨çˆ¬å–æ–‡ç« : {article_info['title']}")
    print(f"   URL: {url}")

    try:
        response = requests.get(url, timeout=15)
        response.raise_for_status()

        soup = BeautifulSoup(response.content, 'html.parser')

        # å¾ JSON-LD çµæ§‹åŒ–æ•¸æ“šä¸­æå–æ¨™é¡Œ(æ›´æº–ç¢º)
        json_ld = soup.find('script', type='application/ld+json')
        title = article_info['title']
        if json_ld:
            try:
                data = json.loads(json_ld.string)
                if 'headline' in data:
                    title = data['headline']
            except:
                pass

        # å¦‚æœ JSON-LD æ²’æœ‰,å˜—è©¦å¾ meta æ¨™ç±¤æˆ– h2 æå–
        if not title or title == article_info['title']:
            meta_title = soup.find('meta', property='og:title')
            if meta_title:
                title = meta_title.get('content', article_info['title'])
            else:
                # å°‹æ‰¾æ–‡ç« æ¨™é¡Œ h2
                title_h2 = soup.find('h2')
                if title_h2:
                    title = title_h2.get_text().strip()

        print(f"  âœ“ æ¨™é¡Œ: {title}")

        # æå–ç™¼å¸ƒæ—¥æœŸ
        date_match = re.search(r'(\d{4})/(\d{2})/(\d{2})', url)
        if date_match:
            publish_date = f"{date_match.group(1)}-{date_match.group(2)}-{date_match.group(3)}"
        else:
            publish_date = article_info.get('date', 'æœªçŸ¥æ—¥æœŸ')
        print(f"  âœ“ æ—¥æœŸ: {publish_date}")

        # æå–æ¨™ç±¤ - å¾æ–‡ç« åº•éƒ¨çš„æ¨™ç±¤å€åŸŸ
        tags = []
        # å°‹æ‰¾åŒ…å«æ¨™ç±¤çš„ section æˆ– div
        tag_links = soup.find_all('a', href=re.compile(r'/tag/'))
        seen_tags = set()
        for tag_elem in tag_links:
            tag_text = tag_elem.get_text().strip()
            if tag_text and tag_text not in seen_tags and len(tag_text) < 50:
                tags.append(tag_text)
                seen_tags.add(tag_text)
        print(f"  âœ“ æ¨™ç±¤: {', '.join(tags) if tags else 'ç„¡'}")

        # æå–æ–‡ç« å…§å®¹ - å°‹æ‰¾ä¸»è¦å…§å®¹å€åŸŸ
        # é»éƒ¨è½çš„æ–‡ç« å…§å®¹é€šå¸¸åœ¨ç‰¹å®šçš„ div ä¸­
        content_elem = None

        # æ–¹æ³•1: å°‹æ‰¾åŒ…å«æ–‡ç« å…§å®¹çš„ä¸»è¦ div
        main_content = soup.find('div', class_='post-item')
        if main_content:
            # ç§»é™¤ä¸éœ€è¦çš„éƒ¨åˆ†
            for unwanted in main_content.find_all(['header', 'footer', 'nav', 'aside']):
                unwanted.decompose()
            content_elem = main_content

        # æ–¹æ³•2: å¦‚æœæ‰¾ä¸åˆ°,å˜—è©¦å…¶ä»–é¸æ“‡å™¨
        if not content_elem:
            content_elem = soup.find('article') or soup.find('div', class_='content')

        if not content_elem:
            print(f"  âš ï¸  ä½¿ç”¨æ•´å€‹ body ä½œç‚ºå…§å®¹")
            content_elem = soup.find('body')

        if not content_elem:
            print(f"  âŒ æ‰¾ä¸åˆ°æ–‡ç« å…§å®¹")
            return None

        # ç§»é™¤é é¦–ã€é å°¾ã€å´é‚Šæ¬„ç­‰
        for unwanted_class in ['navbar', 'header', 'footer', 'sidebar', 'side-sticky', 'page-footer', 'navbar-top']:
            for elem in content_elem.find_all(class_=re.compile(unwanted_class, re.I)):
                elem.decompose()

        # è½‰æ›ç‚º Markdown
        markdown_content = html_to_markdown(str(content_elem), url)

        # åˆ¤æ–·åˆ†é¡
        category = 'Other'
        for tag in tags:
            for key, value in CATEGORY_MAPPING.items():
                if key.lower() in tag.lower():
                    category = value
                    break
            if category != 'Other':
                break

        return {
            'title': title,
            'date': publish_date,
            'tags': tags,
            'category': category,
            'content': markdown_content,
            'url': url
        }

    except requests.exceptions.RequestException as e:
        print(f"  âŒ è«‹æ±‚å¤±æ•—: {e}")
        return None
    except Exception as e:
        print(f"  âŒ è™•ç†å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return None

def save_article_to_markdown(article):
    """å°‡æ–‡ç« ä¿å­˜ç‚º Markdown æª”æ¡ˆ"""
    # ç”Ÿæˆæª”æ¡ˆåç¨±
    filename = sanitize_filename(article['title'])
    if not filename:
        # å¾ URL æå–æª”å
        url_parts = article['url'].rstrip('/').split('/')
        filename = url_parts[-1] if url_parts else f"article_{int(time.time())}"

    filepath = os.path.join(OUTPUT_DIR, f"{filename}.md")

    # ç¢ºä¿ç›®éŒ„å­˜åœ¨
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    # ç”Ÿæˆ Markdown å…§å®¹
    md_content = f"""# {article['title']}

> **åŸæ–‡ç™¼å¸ƒæ—¥æœŸ:** {article['date']}
> **åŸæ–‡é€£çµ:** {article['url']}
> **æ¨™ç±¤:** {', '.join(article['tags']) if article['tags'] else 'ç„¡'}

---

{article['content']}

---

*æœ¬æ–‡ç« å¾é»éƒ¨è½é·ç§»è‡³ Writerside*
"""

    # ä¿å­˜æª”æ¡ˆ
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(md_content)

    print(f"  âœ“ å·²ä¿å­˜: {filepath}")

    return {
        'filename': f"{filename}.md",
        'category': article['category'],
        'title': article['title']
    }

def main():
    """ä¸»ç¨‹å¼"""
    print("=" * 60)
    print("é»éƒ¨è½æ–‡ç« é·ç§»å·¥å…·")
    print("=" * 60)

    all_article_links = []

    # ç¬¬ä¸€æ­¥:çˆ¬å–æ‰€æœ‰é é¢çš„æ–‡ç« åˆ—è¡¨
    print("\nğŸ” ç¬¬ä¸€éšæ®µ:æ”¶é›†æ‰€æœ‰æ–‡ç« é€£çµ...")
    for page_num in range(1, TOTAL_PAGES + 1):
        links = get_article_links_from_page(page_num)
        all_article_links.extend(links)
        time.sleep(1)  # é¿å…è«‹æ±‚éå¿«

    print(f"\nâœ“ ç¸½å…±æ‰¾åˆ° {len(all_article_links)} ç¯‡æ–‡ç« ")

    # å»é‡
    unique_links = {}
    for link in all_article_links:
        unique_links[link['url']] = link

    all_article_links = list(unique_links.values())
    print(f"âœ“ å»é‡å¾Œå‰©é¤˜ {len(all_article_links)} ç¯‡æ–‡ç« ")

    # ç¬¬äºŒæ­¥:çˆ¬å–æ¯ç¯‡æ–‡ç« çš„å…§å®¹
    print("\nğŸ“š ç¬¬äºŒéšæ®µ:çˆ¬å–æ–‡ç« å…§å®¹...")
    articles_info = []

    for idx, article_link in enumerate(all_article_links, 1):
        print(f"\n[{idx}/{len(all_article_links)}]")
        article = fetch_article(article_link)
        if article:
            file_info = save_article_to_markdown(article)
            articles_info.append(file_info)

        # é¿å…è«‹æ±‚éå¿«
        time.sleep(2)

    # ç¢ºä¿ç›®éŒ„å­˜åœ¨
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    # ä¿å­˜æ–‡ç« è³‡è¨Šä¾›å¾ŒçºŒæ›´æ–° hi.tree ä½¿ç”¨
    info_file = os.path.join(OUTPUT_DIR, '_articles_info.json')
    with open(info_file, 'w', encoding='utf-8') as f:
        json.dump(articles_info, f, ensure_ascii=False, indent=2)

    print("\n" + "=" * 60)
    print(f"âœ… å®Œæˆ! å…±è™•ç† {len(articles_info)} ç¯‡æ–‡ç« ")
    print(f"ğŸ“ æ–‡ç« ä¿å­˜åœ¨: {OUTPUT_DIR}")
    print(f"ğŸ–¼ï¸  åœ–ç‰‡ä¿å­˜åœ¨: {IMAGES_DIR}")
    print(f"ğŸ“‹ æ–‡ç« è³‡è¨Š: {info_file}")
    print("=" * 60)

if __name__ == "__main__":
    main()

